{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classifiers (scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors : Rafsanjani Muhammod & Yeazullah Aziz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Avoiding warning\n",
    "import warnings\n",
    "def warn(*args, **kwargs): pass\n",
    "warnings.warn = warn\n",
    "# _______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing essential library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Essential Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# _____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(278, 21)\n",
      "\n",
      "             0          1          2          3          4          5   \\\n",
      "0     31.878947   0.436414  14.864925  42.252833  10.276103   9.144008   \n",
      "1     12.773493   0.857822   6.595729   5.183923   4.495137   7.829791   \n",
      "2     29.506459   1.377185  19.167023  17.223346  17.435778  29.941994   \n",
      "3     45.474052   2.171144  27.655692  22.063534  13.254630  35.628275   \n",
      "4     27.059732   5.266138  24.234508  12.049606  13.223784  25.662224   \n",
      "5     26.648576   7.201495  19.412428  22.140922  10.964234  27.467605   \n",
      "6     29.119535   4.772717  18.706092   9.363417  10.772448  51.772431   \n",
      "7     90.500000   2.000000  34.500000  42.000000  27.500000  46.500000   \n",
      "8     16.419771   0.923602  10.350049  11.182768   5.879438  13.911936   \n",
      "9      8.748706   0.313611   3.006630   4.761503   1.893849   4.508689   \n",
      "10    29.974180   3.232539  19.795971  18.808486  11.644253  21.940464   \n",
      "11    55.917130   2.164977  27.786322  30.427384  19.944195  46.601618   \n",
      "12     9.516898   0.200129   4.102533   4.988537   4.525195   5.123084   \n",
      "13    31.124108   1.401671  22.611352  20.885825  13.658657  23.818998   \n",
      "14     9.409200   2.888764   7.126092   8.338050   7.804591   6.710747   \n",
      "15    30.000000   0.000000   9.000000   7.000000   5.000000  22.000000   \n",
      "16    29.433565   3.505625  19.031125  14.829930  10.101204  32.081270   \n",
      "17    11.204090   0.996893  20.411840  25.983695  18.640679  15.689049   \n",
      "18    28.023893   1.213780  18.518705  17.596743  11.355011  29.449666   \n",
      "19    10.608094   1.296899   9.678177  10.589293   9.097699  11.727422   \n",
      "20     3.500000   2.000000   0.500000   1.000000   1.500000   1.000000   \n",
      "21    18.728560   0.880944  14.266440  12.838985   7.952373  19.718093   \n",
      "22     7.878394   0.000000   0.722663   0.630392   1.759593  16.373235   \n",
      "23    62.030309   3.876600  34.182684  28.502334  19.952895  49.219356   \n",
      "24    17.208633   2.634892  13.403469  14.378830   9.033494  16.724035   \n",
      "25    14.000000   2.000000   5.000000   1.000000   1.000000   9.000000   \n",
      "26    41.799091   3.484240  24.508045  20.591219  14.637986  29.880614   \n",
      "27    16.814381   0.793395  12.381147  12.653434   7.208628  16.678740   \n",
      "28    15.185230   0.000000   3.618992   5.443848   0.097937  11.194652   \n",
      "29     8.200289   1.856511   3.291416   2.655031   3.965002   6.131192   \n",
      "..          ...        ...        ...        ...        ...        ...   \n",
      "248  124.389246   0.172626  46.063459  53.874216  22.238598  76.110825   \n",
      "249   22.934840   1.448894   9.570754   8.781046   1.899455   8.826469   \n",
      "250    2.275179   0.000000   0.398034   0.034602   1.501231   2.731793   \n",
      "251   30.104910   1.164007  23.896370  24.508159  17.435447  30.154862   \n",
      "252    9.456741   4.178321   4.855538   5.575179   1.991322   5.663416   \n",
      "253   15.142981   2.051510  10.937240  12.162304  13.336698  11.083566   \n",
      "254    7.000000   1.500000   0.000000   0.000000   1.500000   7.500000   \n",
      "255   16.357153   3.507617  11.796904   9.782164   7.511679  17.426397   \n",
      "256   10.798229   0.547078   3.465896   1.992843   4.192455   8.435779   \n",
      "257    2.631887   0.418413   1.736242   0.220523   2.532374   3.949971   \n",
      "258    8.229297   0.901293   4.369592   6.881591   2.556971   5.913332   \n",
      "259   31.531619   9.412106  28.001753  23.879145  15.471752  65.120540   \n",
      "260    9.891778   1.063529   6.760600   8.242649   4.113700  12.940859   \n",
      "261   19.266025   1.130491   7.774937   8.585691   1.658805   6.616826   \n",
      "262   30.233875  17.474189  30.545211  20.529689  14.775955  65.520344   \n",
      "263   48.616531   2.889448  32.133396  16.181317  17.417271  56.893856   \n",
      "264   28.003891   1.718347  19.882716  25.801070  10.259633  17.460976   \n",
      "265   11.182267   1.375532  10.174284  14.174082   7.292537   6.283834   \n",
      "266   17.475558   1.061139   8.538175  18.052600   4.947363  11.167212   \n",
      "267   17.072799   0.830625   8.834354   7.492923   5.748905  15.712549   \n",
      "268   30.354527   1.309463  22.814991  23.724757  16.655051  31.315168   \n",
      "269    7.540789   1.103260   3.700105   6.837935   2.091725   5.468799   \n",
      "270   37.122040   0.688216  24.435579  13.557403  18.587103  40.690123   \n",
      "271   24.007569   3.871937  15.876603  15.303825  17.093432  20.671707   \n",
      "272    3.061815   1.475419   1.725519   1.000000   2.176101   4.487867   \n",
      "273   20.328607   0.940953  11.001388  17.045312   4.350877  10.031379   \n",
      "274   41.253185   1.662685  27.304351  26.998738  19.059946  35.608158   \n",
      "275    8.090764   0.333325   6.816283  10.235265   2.112069   3.845509   \n",
      "276    4.712633   1.400182   0.822081   1.827789   3.782965   0.510636   \n",
      "277    6.317053   5.012890   2.803340   4.743114   1.721591   4.635015   \n",
      "\n",
      "            6          7          8          9  ...         11         12  \\\n",
      "0     2.957656  13.618921  22.605322  23.820069 ...  11.762420   3.304842   \n",
      "1     0.509491   6.230910   6.832383   9.858047 ...   7.907631   7.358403   \n",
      "2     3.730435  23.067585  22.055324  24.021771 ...  24.413899  12.772484   \n",
      "3     4.589759  25.428206  24.063574  30.738867 ...  21.895481  18.190512   \n",
      "4     1.227263  16.416353  13.340650  23.446813 ...  17.012233  23.632676   \n",
      "5     6.249618  21.660829  17.389375  25.754882 ...  18.486434  22.337281   \n",
      "6     0.590906  22.453437  14.297445  28.524012 ...  15.999588  13.701873   \n",
      "7    17.000000  42.500000  24.000000  79.500000 ...  27.000000  34.000000   \n",
      "8     1.821375  11.385246   9.798162  14.536601 ...   9.003433  10.239091   \n",
      "9     0.648948   3.312956   2.754154   6.880549 ...   1.598568   1.380729   \n",
      "10    6.171095  22.878370  16.821263  38.709657 ...  13.037691  15.879432   \n",
      "11    5.597017  29.352018  23.274549  37.323073 ...  28.424399  21.116536   \n",
      "12    2.239406   4.226955   5.496089   1.388491 ...   4.026082   2.414103   \n",
      "13    4.032605  20.445754  18.669454  28.468802 ...  15.601069  17.423417   \n",
      "14    2.376401  10.079741   9.478943  14.408974 ...   6.345054   5.482089   \n",
      "15    1.000000   9.000000  12.000000  24.000000 ...  10.000000   1.000000   \n",
      "16    9.223711  19.171717  19.160272  22.965008 ...  17.145750  18.125036   \n",
      "17    2.970032  16.571940  12.814431  18.689993 ...  17.429212  10.670658   \n",
      "18    3.528770  17.969424  15.019329  25.156781 ...  14.832549  12.175126   \n",
      "19    2.474348   9.177967   8.231110  12.792159 ...   8.782537   6.782649   \n",
      "20    0.000000   4.500000   1.000000   4.500000 ...   1.000000   0.500000   \n",
      "21    1.962553  10.898233  10.672790  15.508705 ...   7.555623  10.196841   \n",
      "22    0.441828   3.929370   2.339625   6.743675 ...   4.500594   2.493292   \n",
      "23    5.516421  33.551722  24.357458  45.822568 ...  28.877307  24.077493   \n",
      "24    4.572859  15.882376  12.880601  24.696016 ...   8.603455   9.566684   \n",
      "25    0.000000   3.000000   4.000000  13.000000 ...   8.000000   9.000000   \n",
      "26    4.656481  25.754576  17.208084  33.149172 ...  19.675759  17.244976   \n",
      "27    1.657054   8.438727  10.259314  11.732512 ...   6.704558   7.917051   \n",
      "28    0.097937   3.472190  16.460408   4.365743 ...   1.907713   4.808057   \n",
      "29    1.332384   5.951721   6.051196   8.247525 ...   5.795242   2.049889   \n",
      "..         ...        ...        ...        ... ...        ...        ...   \n",
      "248   5.294059  40.886482  51.870857  75.387114 ...  35.196316  14.969533   \n",
      "249   2.422936   6.261505   6.823316  15.027661 ...   4.163068   2.174035   \n",
      "250   0.573579   1.359968   6.017739   1.946883 ...   1.758274   1.390197   \n",
      "251   4.295513  31.527793  21.556833  44.156689 ...  21.360663  18.045489   \n",
      "252   2.479958   1.882742   2.215831   5.269055 ...   1.241307   4.218739   \n",
      "253   2.329881  13.989877  14.068873  19.645077 ...  11.559984   8.288623   \n",
      "254   0.500000   4.000000   3.000000   7.500000 ...   0.000000   0.000000   \n",
      "255   6.991906  11.468162   8.103585  15.654083 ...   6.942601  10.627107   \n",
      "256   0.340565   6.376565   4.598234   7.221580 ...   1.705921   0.000000   \n",
      "257   0.218696   4.168192   0.000000   3.549530 ...   0.184028   0.000000   \n",
      "258   2.124517   6.224174   6.186536  11.035811 ...   2.980999   3.758479   \n",
      "259   4.467768  19.628881  22.427760  28.620746 ...  19.099746  18.977847   \n",
      "260   1.137475   6.390223   6.307879   7.213921 ...   5.183218   4.252364   \n",
      "261   2.152735   6.836234   6.568352  16.621956 ...   6.560606   4.173425   \n",
      "262   2.717102  16.211105  17.936735  22.444359 ...  18.939561  30.011434   \n",
      "263   6.504133  27.826642  21.088777  33.652976 ...  39.240197  18.021889   \n",
      "264   4.469974  10.878794  10.716317  27.205664 ...   7.101898  12.468127   \n",
      "265   2.041263   5.878167   9.639627   9.926487 ...   8.111750   9.989217   \n",
      "266   3.257303   9.833016  16.535949  18.401333 ...   7.401277   9.133795   \n",
      "267   3.105667   8.249924   9.094696  13.736098 ...   7.024975   7.674594   \n",
      "268   4.390719  32.876376  22.301110  44.522090 ...  22.545305  18.203085   \n",
      "269   2.388460   6.754317   7.750213   8.943494 ...   3.325885   2.981585   \n",
      "270   5.013206  15.038639  15.595770  28.792507 ...  20.704859   8.603426   \n",
      "271   6.112164  23.874307  14.793571  35.275519 ...  12.308957  12.096128   \n",
      "272   0.000000   2.287268   0.000000   4.238423 ...   0.213011   0.000000   \n",
      "273   2.571561   9.422501  11.132179  18.351313 ...   5.191226   6.756515   \n",
      "274   5.260675  26.259621  24.852537  38.024891 ...  22.665054  21.118775   \n",
      "275   2.133378   8.951968   5.916079  10.292114 ...   6.402733   1.166797   \n",
      "276   0.934270   2.959759   6.465208  21.703019 ...   2.551930   6.550973   \n",
      "277   1.764842   4.188473   5.442524   5.506024 ...   2.479256   3.686578   \n",
      "\n",
      "            13         14         15         16         17         18  \\\n",
      "0    11.040083   6.827287  17.541407  13.663309  23.124199   0.841823   \n",
      "1     5.230102   6.632662   9.818324  11.899385  15.539906   0.802143   \n",
      "2    14.999995  15.819657  34.166990  29.487096  21.799881   2.238363   \n",
      "3    15.964934  16.682218  29.771854  38.712343  35.624337   4.421584   \n",
      "4     9.300391  18.733020  23.912026  22.049993  29.353605   6.099477   \n",
      "5    13.643306  15.825133  31.899009  32.803701  30.988117   6.166444   \n",
      "6     9.772717   5.883828  28.995002  28.746565  28.292224   1.000000   \n",
      "7    21.000000  50.500000  66.500000  51.000000  58.500000  10.000000   \n",
      "8     7.282626   7.662447  10.158416  14.901425  14.084472   2.537779   \n",
      "9     3.983211   7.762739   3.915098   3.881487   4.006463   0.320941   \n",
      "10   13.086607  20.527252  22.510262  16.829556  23.197093   2.288294   \n",
      "11   20.071537  21.375529  35.411735  42.093334  35.361695   4.120429   \n",
      "12    1.894125   2.021570   5.746451   6.858210   6.969856   1.124568   \n",
      "13   13.081720  17.618295  19.074519  24.333297  25.819600   4.057952   \n",
      "14    5.010214   5.323018   9.093048   7.087312  10.222282   1.604050   \n",
      "15   23.000000   5.000000  13.000000  11.000000   7.000000   3.000000   \n",
      "16   15.407028  17.427836  21.470474  22.740239  22.988303   6.396602   \n",
      "17   12.485455   8.901322  14.211832  12.360842  14.297911   1.706987   \n",
      "18   11.218623  13.825730  19.762335  23.210426  22.739679   3.732737   \n",
      "19    6.334453   7.479377  11.789404  11.758441  11.591263   3.208637   \n",
      "20    0.000000   1.500000   4.000000   2.500000   6.000000   0.000000   \n",
      "21    4.779371   6.332490  11.014715  17.393482  14.743054   2.894116   \n",
      "22    1.650240   0.862717   9.792214   4.842245   4.032818   1.348500   \n",
      "23   18.264251  24.839331  37.818385  46.454861  47.215040   5.466389   \n",
      "24    9.004378  13.716093  14.073495  12.144620  14.956607   2.189111   \n",
      "25    6.000000   4.000000  12.000000   6.000000   7.000000   4.000000   \n",
      "26   13.476076  18.301284  23.630129  26.773479  32.365092   4.731644   \n",
      "27    6.278316   5.417172  12.116865  16.065471  12.057913   2.147864   \n",
      "28    4.330298   2.953040   6.123404   5.096096  11.249319   0.195874   \n",
      "29    2.338184   2.363287   8.319704   6.016922   6.126070   1.275953   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "248  62.503138  46.532291  59.903903  47.754593  43.939895   6.733377   \n",
      "249   9.353054  12.048591   6.522661   8.358267   8.172523   2.283235   \n",
      "250   0.493681   7.983894   2.434460   1.977025   1.940098   0.199022   \n",
      "251  19.904688  21.421608  34.086436  29.920104  35.850703   2.684970   \n",
      "252   2.706751   8.258543   2.933766   3.622391   3.819396   2.332179   \n",
      "253   7.575656  10.493977  16.435304  11.529431  16.417327   2.300947   \n",
      "254   0.000000   1.500000   1.500000   3.000000   4.500000   0.000000   \n",
      "255   7.012573  11.188181  14.361907   8.958387  13.482256   4.819140   \n",
      "256   1.081191   4.143209   4.726475   4.108084   1.842332   2.247638   \n",
      "257   0.887323   1.000000   2.755444   1.388991   3.362911   0.000000   \n",
      "258   3.959314   7.625730   5.958495   5.419310   6.056779   0.843904   \n",
      "259  14.634964  13.511393  46.641868  28.293350  18.680414   5.174663   \n",
      "260   3.662464   5.054303   6.967471  10.617388  10.224804   1.394032   \n",
      "261  11.134760  11.013103   7.930183   7.672240   7.727995   2.330191   \n",
      "262  12.694026  10.514463  40.249252  36.566208  22.843165   5.922808   \n",
      "263  20.050109  21.107134  36.764960  38.120279  32.272687   9.686281   \n",
      "264  10.045035  12.851233  23.284148  14.254059  17.474913   3.895959   \n",
      "265   8.028108   6.491426   9.153489   7.387854   7.788523   0.475248   \n",
      "266   9.735884  13.340701  12.761160   8.418122  11.597348   3.290583   \n",
      "267   7.885719  11.104351  10.328566   9.067405   7.805965   2.835786   \n",
      "268  20.369799  20.814830  35.511795  31.954508  36.814599   2.502489   \n",
      "269   3.918510   8.822954   6.575558   5.614446   5.555061   1.550017   \n",
      "270  12.479624  14.583105  28.822291  25.184128  23.313276   5.205664   \n",
      "271   8.893267  19.012332  19.761117  14.412219  23.100989   4.469981   \n",
      "272   3.338972   1.000000   1.250116   0.580374   2.905215   0.274714   \n",
      "273   7.800665  15.405596  10.336453   8.940383  11.087188   2.089405   \n",
      "274  18.790415  19.690950  27.633457  40.343920  34.588802   5.173234   \n",
      "275   2.326062   8.952279   4.213363   4.378337   6.646777   0.381568   \n",
      "276   2.143037   4.125674  11.502283   6.317738   4.919783   2.304603   \n",
      "277   3.298381   4.289046   4.382953   4.981193   4.571578   0.633116   \n",
      "\n",
      "            19  20  \n",
      "0     4.193320  -1  \n",
      "1     3.276656  -1  \n",
      "2    11.937721  -1  \n",
      "3    11.880077  -1  \n",
      "4    14.797856  -1  \n",
      "5    14.404472  -1  \n",
      "6    12.297428  -1  \n",
      "7    21.500000  -1  \n",
      "8     5.825278  -1  \n",
      "9     2.395156  -1  \n",
      "10    6.826142  -1  \n",
      "11   18.961684  -1  \n",
      "12    4.017610  -1  \n",
      "13   11.554390  -1  \n",
      "14    4.855885  -1  \n",
      "15    9.000000  -1  \n",
      "16   13.716242  -1  \n",
      "17   13.325879  -1  \n",
      "18    8.776089  -1  \n",
      "19    5.697948  -1  \n",
      "20    2.000000  -1  \n",
      "21    5.762703  -1  \n",
      "22    0.502892  -1  \n",
      "23   19.904820  -1  \n",
      "24    6.259495  -1  \n",
      "25    6.000000  -1  \n",
      "26   13.178515  -1  \n",
      "27    5.593760  -1  \n",
      "28    0.195874  -1  \n",
      "29    4.253174  -1  \n",
      "..         ...  ..  \n",
      "248  19.080687   1  \n",
      "249   3.400859   1  \n",
      "250   0.523776   1  \n",
      "251   9.635971   1  \n",
      "252   0.993018   1  \n",
      "253  13.146648   1  \n",
      "254   1.500000   1  \n",
      "255   8.941937   1  \n",
      "256   0.914297   1  \n",
      "257   0.546927   1  \n",
      "258   2.693732   1  \n",
      "259  13.116168   1  \n",
      "260   2.844852   1  \n",
      "261   2.704174   1  \n",
      "262  15.412857   1  \n",
      "263  17.061099   1  \n",
      "264   7.436631   1  \n",
      "265   5.484195   1  \n",
      "266   4.963489   1  \n",
      "267   7.375263   1  \n",
      "268   9.983461   1  \n",
      "269   4.318317   1  \n",
      "270  20.082637   1  \n",
      "271  12.186918   1  \n",
      "272   1.237854   1  \n",
      "273   4.139329   1  \n",
      "274  15.979330   1  \n",
      "275   2.355134   1  \n",
      "276   1.527400   1  \n",
      "277   2.985759   1  \n",
      "\n",
      "[278 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "D = pd.read_csv('/home/rafsanjani/Desktop/monodata.csv', header=None)\n",
    "D = D.drop_duplicates() # Return : each row are unique value\n",
    "#   print(D.shape) # Return : row, column\n",
    "# ____________________________________________________________________\n",
    "\n",
    "print(D.shape)\n",
    "print()\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide features ( X ) and classes ( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 31.87894693   0.43641389  14.86492535 ...,  23.1241993    0.84182345\n",
      "    4.19332   ]\n",
      " [ 12.77349297   0.85782219   6.59572893 ...,  15.53990602   0.80214332\n",
      "    3.27665617]\n",
      " [ 29.50645867   1.37718472  19.16702346 ...,  21.79988149   2.23836279\n",
      "   11.93772103]\n",
      " ..., \n",
      " [  8.09076416   0.33332467   6.81628278 ...,   6.64677746   0.38156841\n",
      "    2.35513374]\n",
      " [  4.71263264   1.4001818    0.82208071 ...,   4.91978346   2.30460257\n",
      "    1.52740013]\n",
      " [  6.31705298   5.01289044   2.80333981 ...,   4.57157784   0.63311594\n",
      "    2.98575905]]\n",
      "\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "# Divide features (X) and classes (y)\n",
    "X = D.iloc[:,0:20].values\n",
    "y = D.iloc[:,20].values\n",
    "# ____________________________________\n",
    "\n",
    "print(X)\n",
    "print()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the missing values with \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Handle the missing values with \"mean\"\n",
    "from sklearn.preprocessing import Imputer\n",
    "X[:, [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]] = \\\n",
    "    Imputer(strategy='mean').fit_transform(X[:, [0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
    "       17, 18, 19]])\n",
    "# __________________________________________________________________________________________________\n",
    "\n",
    "# You can use another stategy = 'median' & stategy = 'most_frequent'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spliting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, train_size=0.75, random_state=0)\n",
    "# __________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "scale = StandardScaler()\n",
    "X_train = scale.fit_transform(X_train)\n",
    "X_test = scale.fit_transform(X_test)\n",
    "# _____________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import BaggingClassifier, \\\n",
    "                             RandomForestClassifier,\\\n",
    "                             AdaBoostClassifier,\\\n",
    "                             GradientBoostingClassifier\n",
    "            \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# _______________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, \\\n",
    "                            log_loss, \\\n",
    "                            classification_report, \\\n",
    "                            confusion_matrix\n",
    "\n",
    "# ____________________________________________________________________________________________________________\n",
    "\n",
    "from pandas_ml import ConfusionMatrix   # I'm using 'pandas_ml' for better confusion matrix than 'scikit-learn'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(n_neighbors=5),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(kernel='rbf', probability=True),\n",
    "    GaussianNB(),\n",
    "    BaggingClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    XGBClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run all classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________\n",
      "Classifier : LogisticRegression\n",
      "Accuracy : 71.429 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         22   9       31\n",
      "1          11  28       39\n",
      "__all__    33  37       70\n",
      "\n",
      "TN = 22\n",
      "FP = 9\n",
      "FN = 11\n",
      "TP = 28\n",
      "\n",
      "___________________________________________\n",
      "Classifier : KNeighborsClassifier\n",
      "Accuracy : 65.714 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         20  11       31\n",
      "1          13  26       39\n",
      "__all__    33  37       70\n",
      "\n",
      "TN = 20\n",
      "FP = 11\n",
      "FN = 13\n",
      "TP = 26\n",
      "\n",
      "___________________________________________\n",
      "Classifier : DecisionTreeClassifier\n",
      "Accuracy : 64.286 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         18  13       31\n",
      "1          12  27       39\n",
      "__all__    30  40       70\n",
      "\n",
      "TN = 18\n",
      "FP = 13\n",
      "FN = 12\n",
      "TP = 27\n",
      "\n",
      "___________________________________________\n",
      "Classifier : SVC\n",
      "Accuracy : 70.000 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         22   9       31\n",
      "1          12  27       39\n",
      "__all__    34  36       70\n",
      "\n",
      "TN = 22\n",
      "FP = 9\n",
      "FN = 12\n",
      "TP = 27\n",
      "\n",
      "___________________________________________\n",
      "Classifier : GaussianNB\n",
      "Accuracy : 57.143 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         12  19       31\n",
      "1          11  28       39\n",
      "__all__    23  47       70\n",
      "\n",
      "TN = 12\n",
      "FP = 19\n",
      "FN = 11\n",
      "TP = 28\n",
      "\n",
      "___________________________________________\n",
      "Classifier : BaggingClassifier\n",
      "Accuracy : 68.571 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         21  10       31\n",
      "1          12  27       39\n",
      "__all__    33  37       70\n",
      "\n",
      "TN = 21\n",
      "FP = 10\n",
      "FN = 12\n",
      "TP = 27\n",
      "\n",
      "___________________________________________\n",
      "Classifier : RandomForestClassifier\n",
      "Accuracy : 65.714 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         23   8       31\n",
      "1          16  23       39\n",
      "__all__    39  31       70\n",
      "\n",
      "TN = 23\n",
      "FP = 8\n",
      "FN = 16\n",
      "TP = 23\n",
      "\n",
      "___________________________________________\n",
      "Classifier : AdaBoostClassifier\n",
      "Accuracy : 61.429 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         17  14       31\n",
      "1          13  26       39\n",
      "__all__    30  40       70\n",
      "\n",
      "TN = 17\n",
      "FP = 14\n",
      "FN = 13\n",
      "TP = 26\n",
      "\n",
      "___________________________________________\n",
      "Classifier : GradientBoostingClassifier\n",
      "Accuracy : 78.571 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         26   5       31\n",
      "1          10  29       39\n",
      "__all__    36  34       70\n",
      "\n",
      "TN = 26\n",
      "FP = 5\n",
      "FN = 10\n",
      "TP = 29\n",
      "\n",
      "___________________________________________\n",
      "Classifier : LinearDiscriminantAnalysis\n",
      "Accuracy : 64.286 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         18  13       31\n",
      "1          12  27       39\n",
      "__all__    30  40       70\n",
      "\n",
      "TN = 18\n",
      "FP = 13\n",
      "FN = 12\n",
      "TP = 27\n",
      "\n",
      "___________________________________________\n",
      "Classifier : QuadraticDiscriminantAnalysis\n",
      "Accuracy : 65.714 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         19  12       31\n",
      "1          12  27       39\n",
      "__all__    31  39       70\n",
      "\n",
      "TN = 19\n",
      "FP = 12\n",
      "FN = 12\n",
      "TP = 27\n",
      "\n",
      "___________________________________________\n",
      "Classifier : XGBClassifier\n",
      "Accuracy : 74.286 %\n",
      "\n",
      "Confusion Matrix :\n",
      "Predicted  -1   1  __all__\n",
      "Actual                    \n",
      "-1         24   7       31\n",
      "1          11  28       39\n",
      "__all__    35  35       70\n",
      "\n",
      "TN = 24\n",
      "FP = 7\n",
      "FN = 11\n",
      "TP = 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for classifier in classifiers:\n",
    "\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_artificial = model.predict(X_test) # Predicted\n",
    "\n",
    "    name = classifier.__class__.__name__\n",
    "\n",
    "    TN, FP, FN, TP = confusion_matrix(y_true=y_test, y_pred=y_artificial).ravel()\n",
    "\n",
    "    print('_' * 43)\n",
    "    print('Classifier : {}'.format(name))\n",
    "    print('Accuracy : {0:.3f} %'.format(accuracy_score(y_true=y_test, y_pred=y_artificial)*100.0))\n",
    "    # print('_'*40)\n",
    "\n",
    "    print()\n",
    "    print('Confusion Matrix :')\n",
    "    CM = ConfusionMatrix(y_true = y_test, y_pred = y_artificial)\n",
    "    print(CM)\n",
    "    print()\n",
    "\n",
    "    print('TN = {}'.format(TN))\n",
    "    print('FP = {}'.format(FP))\n",
    "    print('FN = {}'.format(FN))\n",
    "    print('TP = {}'.format(TP))\n",
    "    print()\n",
    "\n",
    "    # CM.print_stats() # For Statistics based-on confusion matrix.\n",
    "\n",
    "# _______________________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
